{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:18:41.536015Z","iopub.status.busy":"2022-10-13T17:18:41.535710Z","iopub.status.idle":"2022-10-13T17:19:03.248029Z","shell.execute_reply":"2022-10-13T17:19:03.246849Z","shell.execute_reply.started":"2022-10-13T17:18:41.535951Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing ./ball-track/pkgs/loguru-0.6.0-py3-none-any.whl\n","Installing collected packages: loguru\n","Successfully installed loguru-0.6.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["%cp -r /kaggle/input/ball-track/ /kaggle/working/ball-track\n","!pip install /kaggle/working/ball-track/pkgs/loguru-0.6.0-py3-none-any.whl"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from src.yolox.exp import get_exp\n","from itertools import accumulate\n","from loguru import logger\n","from src.yolox.ball_interpolation import bt_smooth_tracking, get_cropped_frames\n","from src.yolox.inference import Predictor\n","from src.yolox.utils import get_model_info\n","import torch, glob, os, cv2, time, numpy as np, pandas as pd\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:19:07.245883Z","iopub.status.busy":"2022-10-13T17:19:07.245424Z","iopub.status.idle":"2022-10-13T17:19:09.523122Z","shell.execute_reply":"2022-10-13T17:19:09.521767Z","shell.execute_reply.started":"2022-10-13T17:19:07.245832Z"},"trusted":true},"outputs":[],"source":["IMAGE_EXT = [\".jpg\", \".jpeg\", \".webp\", \".bmp\", \".png\"]\n","\n","def get_ball(predictor, config, video_path, start_time=0):\n","    print(video_path)\n","    cap = cv2.VideoCapture(video_path)\n","    border = 8\n","    fps = cap.get(cv2.CAP_PROP_FPS)\n","    frame_no = 0\n","    save_path = os.path.join(f'./tracking_outputs/{video_path[video_path.rfind(\"/\")+1:-4]}.mp4')\n","    print(f\"Saving to {save_path}\")\n","    vid_writer = cv2.VideoWriter(\n","        save_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (config.crop_frame_size[0], config.crop_frame_size[1])\n","    )\n","    \n","    frames, centers, save_centers = [], [], []\n","    batch_frames = []\n","    batch_num = 0\n","    while True:\n","        ret_val, frame = cap.read()\n","        if ret_val:\n","            timestamp = cap.get(cv2.CAP_PROP_POS_MSEC) / 1000\n","            if timestamp >= start_time:\n","                resized = cv2.resize(frame, (1280, 720))\n","                frame = cv2.copyMakeBorder(resized, border, border, 0, 0, cv2.BORDER_CONSTANT, None, value = 0)\n","                frame_shape = frame.shape\n","                batch_frames.append(frame)\n","                \n","                if len(batch_frames) == config.bt_batch_size:\n","                    outputs, ratio = predictor.batch_inference(batch_frames)\n","                    frames += batch_frames\n","                    batch_num += 1\n","                    if (batch_num % 20) == 0:\n","                          print(batch_num, timestamp)\n","\n","                    for output in outputs:\n","\n","                        if output is not None:\n","                            output = output.cpu().numpy()\n","                        if output is not None:\n","                            center = ((output[0, 2] / ratio + output[0, 0] / ratio) // 2,\n","                                      (output[0, 1] / ratio + output[0, 3] / ratio) // 2)\n","                        else:\n","                            center = None\n","\n","                        centers.append(center)\n","                        batch_frames = []\n","\n","\n","                # if enough frames have been accumulated, smooth ball tracking and get cropped frames\n","                if len(frames) > 300:\n","                    bt_centers = bt_smooth_tracking(centers, n_pts=config.bt_smooth_n_pts, outlier_threshold = config.outlier_thresh)\n","                    bt_centers = list(accumulate(bt_centers, lambda x, y: y or x))\n","                    bt_centers = [c if c is not None else (frame.shape[1] // 2, frame.shape[0] // 2)\n","                                  for c in bt_centers]\n","                    \n","                    subset_cropped_frames = get_cropped_frames(frames, bt_centers, config)\n","                    \n","                    for idx, save_frame in enumerate(subset_cropped_frames):\n","                        vid_writer.write(save_frame)\n","                    save_centers += centers\n","                    frames, centers = [], []\n","                    \n","                frame_no += 1\n","            \n","        else:\n","            \n","            # run last batch\n","            outputs, ratio = predictor.batch_inference(batch_frames)\n","            frames += batch_frames\n","\n","            for output in outputs:\n","\n","                if output is not None:\n","                    output = output.cpu().numpy()\n","                if output is not None:\n","                    center = ((output[0, 2] / ratio + output[0, 0] / ratio) // 2,\n","                              (output[0, 1] / ratio + output[0, 3] / ratio) // 2)\n","                else:\n","                    center = None\n","\n","                centers.append(center)\n","                        \n","            # get rest of the frames\n","            bt_centers = bt_smooth_tracking(centers, n_pts=config.bt_smooth_n_pts, outlier_threshold = config.outlier_thresh)\n","            bt_centers = list(accumulate(bt_centers, lambda x, y: y or x))\n","            bt_centers = [c if c is not None else (frame_shape[1] // 2, frame_shape[0] // 2)\n","                          for c in bt_centers]\n","                    \n","            subset_cropped_frames = get_cropped_frames(frames, bt_centers, config)\n","            save_centers += centers\n","            for save_frame in subset_cropped_frames:\n","                vid_writer.write(save_frame)\n","#             cropped_frames += subset_cropped_frames\n","\n","            break\n","                \n","    cap.release()\n","    vid_writer.release()\n","\n","def main(exp, config, resized_path : str):\n","    exp.test_conf = config.conf\n","    exp.nmsthre = config.nms\n","    \n","    model = exp.get_model()\n","    logger.info(\"Model Summary: {}\".format(get_model_info(model, exp.test_size)))\n","    if config.device == \"gpu\":\n","        model.cuda().half()\n","    model.eval()\n","\n","    ckpt_file = config.yolo_model_ckpt\n","    logger.info(\"loading checkpoint\")\n","    ckpt = torch.load(ckpt_file, map_location=\"cpu\")\n","    # load the model state dict\n","    model.load_state_dict(ckpt[\"model\"])\n","    logger.info(\"loaded checkpoint done.\")\n","\n","    predictor = Predictor(\n","        model, exp, (\"ball\",), None, None,\n","        config.device\n","    )\n","    get_ball(predictor, config, resized_path)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:19:19.155250Z","iopub.status.busy":"2022-10-13T17:19:19.154322Z","iopub.status.idle":"2022-10-13T17:19:19.163833Z","shell.execute_reply":"2022-10-13T17:19:19.159981Z","shell.execute_reply.started":"2022-10-13T17:19:19.155212Z"},"trusted":true},"outputs":[],"source":["class Ball_Track_Config:\n","    def __init__(self):\n","        self.video_path = './9f4df856_0.mp4'\n","        self.yolo_model_ckpt = './models/ball_tracking_model.pth'\n","        self.exp_file = './models/yolox_exp.py'\n","        self.nms = 0.5\n","        self.conf = 0.6\n","        self.device = \"cpu\"\n","        \n","        # ball tracking\n","        self.outlier_thresh = 20\n","        self.bt_smooth_n_pts = 2\n","        self.bt_batch_size = 64\n","\n","        #cropping\n","        self.crop_frame_size = [256, 256]"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["['./data/clips/538438_0.mp4',\n"," './data/clips/a9f16c_6.mp4',\n"," './data/clips/c01561_4.mp4',\n"," './data/clips/121364_9.mp4']"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":[]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:19:26.428761Z","iopub.status.busy":"2022-10-13T17:19:26.428358Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["mkdir: tracking_outputs: File exists\n"]},{"name":"stderr","output_type":"stream","text":["2022-11-09 12:49:54.689 | INFO     | __main__:main:113 - loading checkpoint\n","2022-11-09 12:49:56.437 | INFO     | __main__:main:117 - loaded checkpoint done.\n"]},{"name":"stdout","output_type":"stream","text":["./data/clips/121364_7.mp4\n","Saving to ./tracking_outputs/121364_7.mp4\n"]}],"source":["\n","!mkdir tracking_outputs\n","config = Ball_Track_Config()\n","for vid in glob.glob('./data/clips/*'):\n","    # if ('019d5b34_0.mp4' in vid) | ('019d5b34_1.mp4' in vid):\n","    exp = get_exp(config.exp_file, None)\n","    main(exp, config, vid)\n","torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[34mdata\u001b[m\u001b[m/             \u001b[34mmodels\u001b[m\u001b[m/           submit.ipynb\n","\u001b[34meval\u001b[m\u001b[m/             \u001b[34msrc\u001b[m\u001b[m/              \u001b[34mtracking_outputs\u001b[m\u001b[m/\n"]}],"source":[]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:14:48.885338Z","iopub.status.busy":"2022-10-13T17:14:48.884993Z","iopub.status.idle":"2022-10-13T17:14:48.890109Z","shell.execute_reply":"2022-10-13T17:14:48.888904Z","shell.execute_reply.started":"2022-10-13T17:14:48.885306Z"},"trusted":true},"outputs":[],"source":["torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:03:05.542958Z","iopub.status.busy":"2022-10-13T17:03:05.542437Z","iopub.status.idle":"2022-10-13T17:03:46.278013Z","shell.execute_reply":"2022-10-13T17:03:46.276796Z","shell.execute_reply.started":"2022-10-13T17:03:05.542914Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Processing /kaggle/working/eventpreds4/pkgs/iopath-0.1.10.tar.gz\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from iopath==0.1.10) (4.64.0)\n","Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from iopath==0.1.10) (4.3.0)\n","Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath==0.1.10) (2.5.1)\n","Building wheels for collected packages: iopath\n","  Building wheel for iopath (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31549 sha256=49090219e8ffd484fcc551bedeba428b38b7bbbc453ca9c499da36cdaa19aca9\n","  Stored in directory: /root/.cache/pip/wheels/21/ee/5b/b04f1861559da764b71860f05892744fba9f3684f8e4261d70\n","Successfully built iopath\n","Installing collected packages: iopath\n","Successfully installed iopath-0.1.10\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mProcessing /kaggle/working/eventpreds4/pkgs/av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","Installing collected packages: av\n","Successfully installed av-9.2.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mProcessing /kaggle/working/eventpreds4/pkgs/fvcore-0.1.5.post20220512.tar.gz\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5.post20220512) (1.21.6)\n","Requirement already satisfied: yacs>=0.1.6 in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5.post20220512) (0.1.8)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5.post20220512) (6.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5.post20220512) (4.64.0)\n","Requirement already satisfied: termcolor>=1.1 in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5.post20220512) (1.1.0)\n","Requirement already satisfied: Pillow in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5.post20220512) (9.1.1)\n","Requirement already satisfied: tabulate in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5.post20220512) (0.8.10)\n","Requirement already satisfied: iopath>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from fvcore==0.1.5.post20220512) (0.1.10)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from iopath>=0.1.7->fvcore==0.1.5.post20220512) (4.3.0)\n","Requirement already satisfied: portalocker in /opt/conda/lib/python3.7/site-packages (from iopath>=0.1.7->fvcore==0.1.5.post20220512) (2.5.1)\n","Building wheels for collected packages: fvcore\n","  Building wheel for fvcore (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for fvcore: filename=fvcore-0.1.5.post20220512-py3-none-any.whl size=61288 sha256=0a5708571ef5ae6caf22742eec76ac2648c9930ce8f7ccfe265a11e1e911bd00\n","  Stored in directory: /root/.cache/pip/wheels/91/c8/85/7743398b7036cd1f332790e0de734227ac0e18ba0f297d743a\n","Successfully built fvcore\n","Installing collected packages: fvcore\n","Successfully installed fvcore-0.1.5.post20220512\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["%cp -r /kaggle/input/eventpreds4/ /kaggle/working/eventpreds4\n","!cp /kaggle/working/eventpreds4/pkgs/iopath-0.1.10.whl /kaggle/working/eventpreds4/pkgs/iopath-0.1.10.tar.gz\n","!cp /kaggle/working/eventpreds4/pkgs/fvcore-0.1.5.post20220512.whl /kaggle/working/eventpreds4/pkgs/fvcore-0.1.5.post20220512.tar.gz\n","!pip install /kaggle/working/eventpreds4/pkgs/iopath-0.1.10.tar.gz\n","!pip install /kaggle/working/eventpreds4/pkgs/av-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n","!pip install /kaggle/working/eventpreds4/pkgs/fvcore-0.1.5.post20220512.tar.gz"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:03:46.281187Z","iopub.status.busy":"2022-10-13T17:03:46.280789Z","iopub.status.idle":"2022-10-13T17:03:46.597454Z","shell.execute_reply":"2022-10-13T17:03:46.595462Z","shell.execute_reply.started":"2022-10-13T17:03:46.281147Z"},"trusted":true},"outputs":[],"source":["import torch\n","from tqdm import tqdm\n","from importlib import import_module\n","import os\n","import shutil\n","from glob import glob\n","import pytorchvideo\n","from pytorchvideo.data.ava import AvaLabeledVideoFramePaths\n","import torch.nn as nn\n","\n","from src.SlowFast.data_loader.data_generator import DataLoader\n","from src.SlowFast.utils.training_utils import ModelCheckpoint\n","from src.SlowFast.utils.data_utils import *\n","import pickle as pkl"]},{"cell_type":"code","execution_count":23,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:05:12.514576Z","iopub.status.busy":"2022-10-13T17:05:12.514176Z","iopub.status.idle":"2022-10-13T17:05:12.541383Z","shell.execute_reply":"2022-10-13T17:05:12.540313Z","shell.execute_reply.started":"2022-10-13T17:05:12.514537Z"},"trusted":true},"outputs":[],"source":["import torch\n","\n","class SlowFast_Config:\n","\n","    def __init__(self):\n","        self.resume_training = False\n","\n","        self.loss_fn = 'CrossEntropyLoss'\n","        self.metric = 'AccMetric'\n","        self.architecture = 'slowfast'\n","\n","        self.num_epochs = 200\n","        self.epoch_steps = 1600\n","        self.batch_size = 16\n","        self.grad_accum = 1\n","        self.learning_rate = 0.001\n","        self.window_len = 100  # number of frames in each window sent to model\n","        self.prediction_len = 24  # number of frames for which the model will predict events\n","        self.label_ratios = [0.28, 0.55, 0.12, 0.05]\n","        self.tolerances = [3, 5, 3, 0]\n","        self.img_size = 128\n","        self.size_fact = 256 // self.img_size\n","        self.pred_jump = 24\n","\n","        self.data_mean = [0.45, 0.45, 0.45]\n","        self.data_std = [0.225, 0.225, 0.225]\n","        self.slow_fast_alpha = 4\n","\n","        # augmentation\n","        self.aug = True\n","        self.aug_hflip_p = 0.5\n","        self.aug_scale = [0.7, 0.9]\n","        self.aug_scale_p = 0.2\n","\n","        self.vid_paths = '/kaggle/working/tracking_outputs/'\n","        self.label_csv = 'train.csv'\n","\n","        self.train_vids = ['3c993bd2_0', '3c993bd2_1', '1606b0e6_0', '1606b0e6_1',\n","                           'cfbe2e94_0', 'cfbe2e94_1', '35bd9041_0', '35bd9041_1', '4ffd5986_0']\n","        self.val_vids = ['407c5a9e_1', 'ecf251d4_0', '9a97dae4_1']\n","        self.labels = ['challenge', 'play', 'throwin']\n","        self.label_dict = {v: i for i, v in enumerate(self.labels)}\n","\n","\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","        \n","class SubmissionGenerator:\n","    def __init__(self, config, video_paths):\n","        self.config = config\n","        self.terminal_width = shutil.get_terminal_size((80, 20)).columns\n","\n","        # Model\n","        print(f' Model: {self.config.architecture} '.center(self.terminal_width, '*'), end='\\n\\n')\n","        model_type = import_module('models.' + self.config.architecture)\n","        create_model = getattr(model_type, 'create_model')\n","        self.model = create_model(self.config)\n","\n","        self.model = create_model(self.config)\n","        # print(self.model)\n","        self.config.fold = None\n","        model_checkpoint = ModelCheckpoint(config=self.config, weight_dir='../')\n","        self.model, _, _, _ = model_checkpoint.load(self.model, load_best=True)\n","\n","        print(f' Loading data '.center(self.terminal_width, '*'))\n","\n","        self.video_paths = video_paths\n","\n","    def _generate_submission_file_model(self, model, outfile_prefix=''):\n","        print(f' Predict '.center(self.terminal_width, '*'))\n","        model.eval()\n","        start_int = (self.config.window_len - self.config.prediction_len) // 2\n","\n","        vid_preds = {}\n","        for vid in self.video_paths:\n","            print(vid)\n","\n","            data_loader = DataLoader(self.config)\n","            test_loader = data_loader.create_test_loader(vid)\n","\n","            pred_range = test_loader.dataset.pred_range\n","            preds = np.zeros((int(test_loader.dataset.duration * 25), 4))\n","            sta = (self.config.window_len - self.config.prediction_len) // 2\n","            norm_factor = self.config.prediction_len // self.config.pred_jump\n","\n","            for i, [x, idx] in enumerate(tqdm(test_loader)):\n","                x = [inp.to(self.config.device) for inp in x]\n","                idx = idx.data.numpy()\n","\n","                with torch.autocast('cuda'):\n","                    pred = nn.functional.softmax(model(x), 1).data.cpu().numpy()\n","                    pred = pred[:, :, start_int:start_int + self.config.prediction_len]\n","\n","                for b_pred_idx, b_pred in zip(idx, pred):\n","                    prev = preds[sta:sta+b_pred.shape[1]]\n","                    preds[sta:sta+b_pred.shape[1]] = prev + (b_pred.T[:prev.shape[0]] / norm_factor)\n","                    sta += self.config.pred_jump\n","        \n","            vid_preds[vid[vid.rfind('/') + 1:-4]] = preds\n","            \n","        return vid_preds\n","\n","        # predictions = np.concatenate(preds, 0).transpose(2, 0, 1).reshape(-1, 28)\n","        # sample_submission = pd.read_csv('../data/sample_submission_uncertainty.csv')\n","        #\n","        # # Merge with sample_submission by using series ids\n","        # pred_ids = np.concatenate([[series_id[series_id.find('_') + 1:] + '_' + str(q).ljust(5, '0')\n","        #                             for series_id in self.agg_ids] for q in self.quantiles])\n","        # predictions_df = pd.DataFrame(np.hstack([pred_ids.reshape(-1, 1), predictions]),\n","        #                               columns=['id'] + [f'F{i + 1}' for i in range(28)])\n","        # sample_submission = sample_submission[['id']].merge(predictions_df, how='left',\n","        #                                                     left_on=sample_submission.id.str[:-11], right_on='id')\n","        # sample_submission['id'] = sample_submission['id_x']\n","        # del sample_submission['id_x'], sample_submission['id_y']\n","        #\n","        # # Export\n","        # sample_submission.to_csv(f'{self.sub_dir}/{outfile_prefix}submission.csv.gz', compression='gzip', index=False,\n","        #                          float_format='%.3g')\n","\n","    def generate_submission_file(self):\n","        return self._generate_submission_file_model(self.model)\n","    \n","def find_labels(preds, vid):\n","    last_lab = None\n","    same_preds_probs = []\n","    same_preds_start_idx = 0\n","    vid_labs = []\n","    for idx, pr in enumerate(preds[38:-40].argmax(1)): \n","        if last_lab is None:\n","            if pr != 3:\n","                last_lab = pr\n","                same_preds_probs.append(preds[idx + 38, pr])\n","                same_preds_start_idx = idx + 38\n","        else:\n","            if (pr == last_lab):\n","                same_preds_probs.append(preds[idx + 38, pr])\n","            else:\n","                if len(same_preds_probs) > 1:\n","                    pred_pos = np.average(np.arange(len(same_preds_probs)), weights=same_preds_probs)\n","                    # prob = preds[int(np.floor(pred_pos)) + same_preds_start_idx, last_lab]\n","                    prob = max(same_preds_probs)\n","                    pred_pos = (same_preds_start_idx + pred_pos) * 0.04\n","                    vid_labs.append([vid, pred_pos, config.labels[last_lab], prob])\n","                    same_preds_probs = []\n","                    same_preds_start_idx = 0\n","\n","                    if pr != 3:\n","                        last_lab = pr\n","                        same_preds_probs.append(preds[idx + 38, pr])\n","                        same_preds_start_idx = idx + 38\n","                    else:\n","                        last_lab = None\n","                else:\n","                    if pr != 3:\n","                        last_lab = pr\n","                        same_preds_probs.append(preds[idx + 38, pr])\n","                        same_preds_start_idx = idx + 38\n","                    else:\n","                        last_lab = None\n","                    \n","    return vid_labs"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:03:46.658105Z","iopub.status.busy":"2022-10-13T17:03:46.657746Z","iopub.status.idle":"2022-10-13T17:04:00.281195Z","shell.execute_reply":"2022-10-13T17:04:00.279315Z","shell.execute_reply.started":"2022-10-13T17:03:46.658069Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["******************************* Model: slowfast ********************************\n","\n","********************************* Loading data *********************************\n","*********************************** Predict ************************************\n","/kaggle/working/ball-track/tracking_outputs/019d5b34_1.mp4\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:05<00:00,  2.94s/it]\n"]},{"name":"stdout","output_type":"stream","text":["/kaggle/working/ball-track/tracking_outputs/019d5b34_0.mp4\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2/2 [00:05<00:00,  2.72s/it]\n"]}],"source":["config = SlowFast_Config()\n","import glob\n","vid_paths = glob.glob('./tracking_outputs/*')\n","predictor = SubmissionGenerator(config, vid_paths)\n","vid_preds = predictor.generate_submission_file()"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:05:16.548706Z","iopub.status.busy":"2022-10-13T17:05:16.548319Z","iopub.status.idle":"2022-10-13T17:05:16.558083Z","shell.execute_reply":"2022-10-13T17:05:16.556590Z","shell.execute_reply.started":"2022-10-13T17:05:16.548671Z"},"trusted":true},"outputs":[],"source":["sub_data = []\n","for vid, preds in vid_preds.items():\n","    vid_labs = find_labels(preds, vid)\n","    sub_data += vid_labs"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T17:05:55.591328Z","iopub.status.busy":"2022-10-13T17:05:55.590952Z","iopub.status.idle":"2022-10-13T17:05:55.600376Z","shell.execute_reply":"2022-10-13T17:05:55.599413Z","shell.execute_reply.started":"2022-10-13T17:05:55.591292Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[['019d5b34_1', 5.351899254562475, 'play', 0.9138230681419373],\n"," ['019d5b34_1', 8.493307499301903, 'play', 0.9740211367607117],\n"," ['019d5b34_1', 10.279594628383885, 'play', 0.524573028087616],\n"," ['019d5b34_1', 13.717983955411503, 'play', 0.8791025876998901],\n"," ['019d5b34_1', 17.8988588060504, 'play', 0.4833300709724426],\n"," ['019d5b34_1', 19.122520057823102, 'play', 0.6615259647369385],\n"," ['019d5b34_1', 23.800946161953252, 'play', 0.9545438885688782],\n"," ['019d5b34_1', 24.919404908167778, 'play', 0.8766966462135315],\n"," ['019d5b34_1', 25.595996098606143, 'play', 0.5223039388656616],\n"," ['019d5b34_1', 26.5182817485419, 'play', 0.5261824727058411],\n"," ['019d5b34_0', 2.9386480435913938, 'play', 0.7140445113182068],\n"," ['019d5b34_0', 16.861001777835405, 'play', 0.5850505232810974],\n"," ['019d5b34_0', 19.10974876262489, 'play', 0.8750974535942078],\n"," ['019d5b34_0', 22.622186513770803, 'play', 0.8794339299201965],\n"," ['019d5b34_0', 25.44351352180087, 'play', 0.890841007232666],\n"," ['019d5b34_0', 27.620485138862698, 'play', 0.5671261548995972]]"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["sub_data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-10-13T05:55:11.671039Z","iopub.status.busy":"2022-10-13T05:55:11.670673Z","iopub.status.idle":"2022-10-13T05:55:11.689191Z","shell.execute_reply":"2022-10-13T05:55:11.688280Z","shell.execute_reply.started":"2022-10-13T05:55:11.671009Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","sub_df = pd.DataFrame(sub_data, columns=['video_id', 'time', 'event', 'score'])\n","# Export\n","sub_df.to_csv(\"/kaggle/working/submission.csv\", index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.13 ('DFL')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.13"},"vscode":{"interpreter":{"hash":"c4728ddfe1cf01a240d506b2b58a8619248a6de09eef6aed8bd2b7e95445a005"}}},"nbformat":4,"nbformat_minor":4}
